setwd("C:/Users/Venkatramani/Desktop/Test")
setwd("F:/Git Push/SVM Classification")
setwd("C:/Users/Venkatramani/Desktop/Test")
#setwd("C:/Users/Venkatramani/Desktop/Test")
data_c <- read.csv('classification_data.csv', header = TRUE)
data.new <- data_c[data_c$Cycle!=8,]
library(e1071)
# Convert the two variables 'value0.1' and 'value0.2' into matrix form.
x = matrix(data.new$value.0.1,data.new$value.0.2,nrow = nrow(data.new), ncol = 2)
# Map the target variable 'Result' to y.
y = c(data.new$Result)
# Plot the result. #13 to equalise the x and y lengths.
plot(x, col=(13-y))
dat=data.frame(x=x, y=as.factor(y))
svmfit=svm(y~., data=dat, kernel="radial",scale=FALSE)
plot(svmfit, dat)
summary(svmfit)
pred <- predict(svmfit,x)
system.time(pred <- predict(svmfit,x))
svm_tune <- tune(svm, train.x=x, train.y=y,kernel="radial", ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
system.time(svm_tune <- tune(svm, train.x=x, train.y=y,kernel="radial", ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100))))
summary(svm_tune)
bestmod = svm_tune$best.model
summary(bestmod)
svmfit.aftertune=svm(y~., data=dat, kernel="radial",cost=0.01,gamma=0.5,scale=FALSE)
summary(svmfit.aftertune)
pred.1 <- predict(svmfit.aftertune,x)
system.time(pred.1 <- predict(svmfit.aftertune,x))
library(kernlab)
m <- ksvm(x,y, data = data.train,kernel="rbfdot",C=1)
m
# Set 80% as training set and 20% as test set.
dim(dat)
data.train <- dat[1:1700,]
data.test <- dat[1701:2142,]
# Exclude the y variable from the test set to match the no of predictors.
p <- predict(m,data.test[,-3],type="response")
head(p)
# Cross check if all the Result variable of the test set is predicted.
dim(p)
table <- table(p,data.test$y)
tail(table)
#setwd("C:/Users/Venkatramani/Desktop/Test")
data_c <- read.csv('classification_data.csv', header = TRUE)
data.new <- data_c[data_c$Cycle!=8,]
library(e1071)
# Convert the two variables 'value0.1' and 'value0.2' into matrix form.
x = matrix(data.new$value.0.1,data.new$value.0.2,nrow = nrow(data.new), ncol = 2)
# Map the target variable 'Result' to y.
y = c(data.new$Result)
# Plot the result. #13 to equalise the x and y lengths.
plot(x, col=(13-y))
dat=data.frame(x=x, y=as.factor(y))
svmfit=svm(y~., data=dat, kernel="radial",scale=FALSE)
plot(svmfit, dat)
summary(svmfit)
pred <- predict(svmfit,x)
system.time(pred <- predict(svmfit,x))
svm_tune <- tune(svm, train.x=x, train.y=y,kernel="radial", ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
system.time(svm_tune <- tune(svm, train.x=x, train.y=y,kernel="radial", ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100))))
summary(svm_tune)
bestmod = svm_tune$best.model
summary(bestmod)
svmfit.aftertune=svm(y~., data=dat, kernel="radial",cost=0.01,gamma=0.5,scale=FALSE)
summary(svmfit.aftertune)
pred.1 <- predict(svmfit.aftertune,x)
system.time(pred.1 <- predict(svmfit.aftertune,x))
library(kernlab)
m <- ksvm(x,y, data = data.train,kernel="rbfdot",C=1)
m
# Set 80% as training set and 20% as test set.
dim(dat)
data.train <- dat[1:1700,]
data.test <- dat[1701:2142,]
# Exclude the y variable from the test set to match the no of predictors.
p <- predict(m,data.test[,-3],type="response")
head(p)
# Cross check if all the Result variable of the test set is predicted.
dim(p)
table <- table(p,data.test$y)
tail(table)
